{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd and Variable : about Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd** : automatic differentiation. Backpropagation을 위한 미분 값을 자동으로 계산해 줌. 자동 계산을 위해 사용하는 변수는 torch.autograd package 안에 있는 Variable 변수."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable : data, grad, grad_fn 으로 구성.\n",
    "- data : Tensor형태의 데이터\n",
    "- grad : Data가 거쳐 온 layer에 대한 미분값이 축적됨.\n",
    "- grad_fn : 미분 값을 계산한 함수에 대한 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "a = torch.ones(2,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = Variable(a, requires_grad = True) # requires_grad = True : 이 variable은 graident가 필요함.\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.data)\n",
    "print(a.grad)\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "b = a + 2\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = b **2\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = c.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.data : \n",
      " tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "a.grad : \n",
      " tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "\n",
      "a.grad_fn : \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(\"a.data : \\n\", a.data)\n",
    "print(\"\\na.grad : \\n\", a.grad) # out.backward()를 통해 grad가 생김.\n",
    "print(\"\\na.grad_fn : \\n\", a.grad_fn) # a가 직접 수행한 연산이 없으므로 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.data : \n",
      " tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "\n",
      "b.grad : \n",
      " None\n",
      "\n",
      "b.grad_fn : \n",
      " <AddBackward object at 0x7f329e79e550>\n"
     ]
    }
   ],
   "source": [
    "print(\"b.data : \\n\", b.data)\n",
    "print(\"\\nb.grad : \\n\", b.grad)\n",
    "print(\"\\nb.grad_fn : \\n\", b.grad_fn) # add 연산에 대한 backward를 했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c.data : \n",
      " tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "\n",
      "c.grad : \n",
      " None\n",
      "\n",
      "c.grad_fn : \n",
      " <PowBackward0 object at 0x7f32fc3c8f60>\n"
     ]
    }
   ],
   "source": [
    "print(\"c.data : \\n\", c.data)\n",
    "print(\"\\nc.grad : \\n\", c.grad)\n",
    "print(\"\\nc.grad_fn : \\n\", c.grad_fn) # b의 제곱이었으므로 power 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.data : \n",
      " tensor(36.)\n",
      "\n",
      "out.grad : \n",
      " None\n",
      "\n",
      "out.grad_fn : \n",
      " <SumBackward0 object at 0x7f3308222080>\n"
     ]
    }
   ],
   "source": [
    "print(\"out.data : \\n\", out.data)\n",
    "print(\"\\nout.grad : \\n\", out.grad)\n",
    "print(\"\\nout.grad_fn : \\n\", out.grad_fn) # c의 sum이었음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 또 다른 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z :  tensor([3., 3., 3.], grad_fn=<MulBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3)\n",
    "x = Variable(x, requires_grad = True)\n",
    "y = x**2\n",
    "z = y*3\n",
    "print(\"z : \", z)\n",
    "\n",
    "# x.grad 값에 grad가 곱해져서 나옴.\n",
    "grad = torch.Tensor([0.1, 1, 10])\n",
    "z.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.data : \n",
      " tensor([1., 1., 1.])\n",
      "\n",
      "x.grad : \n",
      " tensor([ 0.6000,  6.0000, 60.0000])\n",
      "\n",
      "x.grad_fn : \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(\"x.data : \\n\", x.data)\n",
    "print(\"\\nx.grad : \\n\", x.grad)\n",
    "print(\"\\nx.grad_fn : \\n\", x.grad_fn) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
